# Path of input and output resources
input:
    train_file: gs://zh_clue_data/clue_datasets/clue_pretrain_0023214.txt
    valid_file: gs://zh_clue_data/clue_datasets/clue_pretrain_0023214.txt
    tokenizer_file: gs://zh_clue_data/models/bert-base-chinese-vocab.txt
    # If pretrained_model_dir is set except for null,
    # the model is loaded from here and `model_param` section is ignored
    pretrained_model_dir: null
output:
    model_dir:       gs://zh_clue_data/models/output/model
    tokenizer_file:  gs://zh_clue_data/models/output/model/tokenizer.json
    tensorboard_dir: gs://zh_clue_data/models/output/tensorboard
    checkpoint_path: gs://zh_clue_data/models/output/ckpt.h5


# model_params is used when pretrained_model_dir is set to null
model_params:
    n_embd: 768
    n_layer: 12
    n_head: 12
    n_ctx: 512

train:
    block_size: 512
    seed: 1234
    num_epochs: 10
    batch_size: 6
    learning_rate: 0.00005
    max_grad_norm: 1.0
    warmup_rate: 0.1
    patience: 3

pred:
    # [Generation Config]
    do_sample: True
    seed: 1234
    max_length: 100
    top_k: 50
    top_p: 0.9
    bad_words:
        - "[UNK]"
        - "[PAD]"
        - "[SEP]"